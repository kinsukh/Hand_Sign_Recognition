{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVAivB_6FyTO",
        "outputId": "6ff19bc8-03b6-47a9-d034-c88f8a76dc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.22.1)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.22.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.9.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kinsukh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "d3134e270fbf49cabcd526ee4e79cd2a",
            "002c96027a07454f8a8707d8bae27f73",
            "a64a696434bb446882408ae3e2549784",
            "8fd0854c9fc244f7838b0a3b05063bff",
            "6573a68b10644dabb7820866fbd41dd0",
            "0f3dcf2a5a28486dbaeeec92223f6e90",
            "66b1de34389f4685a1d525a65d706aea",
            "8cc5db1891a04c958842a50fc7849a5d",
            "9ed38bc043ec4f6dacd76f8aa31bc10b",
            "78bd602ac82943189e79902bf5bbbc65",
            "3a0693ae046a460aa2f7f3b81e6dd3f9"
          ]
        },
        "id": "JSvRVIy4GU5B",
        "outputId": "8dcbf711-c026-4d34-d2bc-05311c2ab99b"
      },
      "outputs": [],
      "source": [
        "# importing necessary dependancies\n",
        "import torch\n",
        "from transformers import (\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        ")\n",
        "import re\n",
        "\n",
        "# defining class NMpipeline to get words from the world wide web \n",
        "class NMPipeline:\n",
        "    def __init__(\n",
        "        self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, use_cuda: bool\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        if self.model.__class__.__name__ not in [\"T5ForConditionalGeneration\"]:\n",
        "            raise AssertionError\n",
        "\n",
        "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
        "            self.model_type = \"t5\"\n",
        "\n",
        "        self.default_generate_kwargs = {\n",
        "            \"max_length\": 1024,\n",
        "            \"num_beams\": 4,\n",
        "            \"length_penalty\": 1.5,\n",
        "            \"no_repeat_ngram_size\": 3,\n",
        "            \"early_stopping\": True,\n",
        "        }\n",
        "\n",
        "    def __call__(self, keywords, **kwargs):\n",
        "        inputs = self._prepare_inputs_for_nm(keywords)\n",
        "        result = \"\"\n",
        "        if not kwargs:\n",
        "            kwargs = self.default_generate_kwargs\n",
        "\n",
        "        for txt in inputs:\n",
        "            input_ids = self._tokenize(\"{} </s>\".format(txt), padding=False)\n",
        "            outputs = self.model.generate(input_ids.to(self.device), **kwargs)\n",
        "            result += self.tokenizer.decode(outputs[0])\n",
        "\n",
        "        result = re.sub(\"<pad>|</s>\", \"\", result)\n",
        "        return result.strip()\n",
        "\n",
        "    def _prepare_inputs_for_nm(self, keywords):\n",
        "        text = str(keywords)\n",
        "        text = text.replace(\",\", \" \")\n",
        "        text = text.replace(\"'\", \"\")\n",
        "        text = text.replace(\"[\", \"\")\n",
        "        text = text.replace(\"]\", \"\")\n",
        "        texts = text.split(\".\")\n",
        "        return texts\n",
        "\n",
        "    def _tokenize(\n",
        "        self,\n",
        "        inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=1024,\n",
        "    ):\n",
        "        inputs = self.tokenizer.encode(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            truncation=truncation,\n",
        "            padding=\"max_length\" if padding else False,\n",
        "            pad_to_max_length=padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return inputs\n",
        "\n",
        "# defining class to take out the relative words to our given keywords\n",
        "class bloomPipeline:\n",
        "    def __init__(\n",
        "        self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, use_cuda: bool\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        if self.model.__class__.__name__ not in [\"T5ForConditionalGeneration\"]:\n",
        "            raise AssertionError\n",
        "\n",
        "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
        "            self.model_type = \"t5\"\n",
        "\n",
        "        self.default_generate_kwargs = {\n",
        "            \"max_length\": 1024,\n",
        "            \"num_beams\": 4,\n",
        "            \"length_penalty\": 1.5,\n",
        "            \"no_repeat_ngram_size\": 3,\n",
        "            \"early_stopping\": True,\n",
        "        }\n",
        "\n",
        "    def __call__(self, keywords, **kwargs):\n",
        "        inputs = self._prepare_inputs_for_k2t(keywords)\n",
        "        result = \"\"\n",
        "        if not kwargs:\n",
        "            kwargs = self.default_generate_kwargs\n",
        "\n",
        "        for txt in inputs:\n",
        "            input_ids = self._tokenize(\"{} </s>\".format(txt), padding=False)\n",
        "            outputs = self.model.generate(input_ids.to(self.device), **kwargs)\n",
        "            result += self.tokenizer.decode(outputs[0])\n",
        "\n",
        "        result = re.sub(\"<pad>|</s>\", \"\", result)\n",
        "        return result.strip()\n",
        "  #replaces all punctuations with space to only take the words\n",
        "    def _prepare_inputs_for_k2t(self, keywords):\n",
        "        text = str(keywords)\n",
        "        text = text.replace(\",\", \"|\")\n",
        "        text = text.replace(\"'\", \"\")\n",
        "        text = text.replace(\"[\", \"\")\n",
        "        text = text.replace(\"]\", \"\")\n",
        "        texts = text.split(\".\")\n",
        "        return texts\n",
        "\n",
        "    def _tokenize(\n",
        "        self,\n",
        "        inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=1024,\n",
        "    ):\n",
        "        inputs = self.tokenizer.encode(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            truncation=truncation,\n",
        "            padding=\"max_length\" if padding else False,\n",
        "            pad_to_max_length=padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqaeCoYQFbvI"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Union\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedTokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "SUPPORTED_TASKS = {\n",
        "    \"bloom\": {\n",
        "        \"impl\": bloomPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"bigscience/bloom\",\n",
        "        },\n",
        "    },\n",
        "    \"bloom-base\": {\n",
        "        \"impl\": bloomPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"bigscience/bloom-base\",\n",
        "        },\n",
        "    },\n",
        "    \"mrm8488/t5-base-finetuned-common_gen\": {\n",
        "        \"impl\": NMPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"bigscience/bloom-lb7\",\n",
        "        },\n",
        "    },\n",
        "    \"bloom-new\": {\n",
        "        \"impl\": NMPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"bigscience/bloom-new\",\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def pipeline(\n",
        "    task: str,\n",
        "    model: Optional = None,\n",
        "    tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
        "    use_cuda: Optional[bool] = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    :param task:\n",
        "    (:obj:`str`):\n",
        "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
        "            - :obj:`\"bloom\"`: will return a :class:`bloomPipeline` which is based on the bloom model based on t5-small\n",
        "            - :obj:`\"bloom-tiny\"`: will return a :class:`bloomPipeline` which is based on the bloom model based on t5-tiny\n",
        "            - :obj:`\"bloom-base\"`: will return a :class:`bloomPipeline` which is based on the bloom model based on t5-base\n",
        "    :param model:\n",
        "    (:obj:`str` or `optional`):\n",
        "            The model that will be used by the pipeline to make predictions.\n",
        "            If not provided, the default for the :obj:`task` will be loaded.\n",
        "    :param tokenizer:\n",
        "    (:obj:`str` or `optional`):\n",
        "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
        "            identifier or an actual pretrained tokenizer inheriting from :class:`~transformers.PreTrainedTokenizer`.\n",
        "            If not provided, the default tokenizer for the given :obj:`model` will be loaded (if it is a string).\n",
        "    :param use_cuda:\n",
        "    (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            Whether or not to use a GPU or not Default: True\n",
        "    :return:\n",
        "    (:class:):\n",
        "            `bloomPipeline`: A Keytotext pipeline for the task.\n",
        "    \"\"\"\n",
        "\n",
        "    if task not in SUPPORTED_TASKS:\n",
        "        raise KeyError(\n",
        "            \"Unknown task {}, available tasks are {}\".format(\n",
        "                task, list(SUPPORTED_TASKS.keys())\n",
        "            )\n",
        "        )\n",
        "\n",
        "    targeted_task = SUPPORTED_TASKS[task]\n",
        "    task_class = targeted_task[\"impl\"]\n",
        "\n",
        "    if model is None:\n",
        "        model = targeted_task[\"default\"][\"model\"]\n",
        "\n",
        "    if tokenizer is None:\n",
        "        if isinstance(model, str):\n",
        "            tokenizer = model\n",
        "        else:\n",
        "\n",
        "            raise Exception(\n",
        "                \"Please provided a PretrainedTokenizer \"\n",
        "                \"class or a path/identifier to a pretrained tokenizer.\"\n",
        "            )\n",
        "    if isinstance(tokenizer, (str, tuple)):\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
        "\n",
        "\n",
        "    if isinstance(model, str):\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
        "\n",
        "    return task_class(model=model, tokenizer=tokenizer, use_cuda=use_cuda)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "94133a3f85babb63055940c6390102746a71cdc6aba789fdba456b77e0bda27b"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "002c96027a07454f8a8707d8bae27f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f3dcf2a5a28486dbaeeec92223f6e90",
            "placeholder": "​",
            "style": "IPY_MODEL_66b1de34389f4685a1d525a65d706aea",
            "value": ""
          }
        },
        "0f3dcf2a5a28486dbaeeec92223f6e90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a0693ae046a460aa2f7f3b81e6dd3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6573a68b10644dabb7820866fbd41dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b1de34389f4685a1d525a65d706aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78bd602ac82943189e79902bf5bbbc65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cc5db1891a04c958842a50fc7849a5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8fd0854c9fc244f7838b0a3b05063bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78bd602ac82943189e79902bf5bbbc65",
            "placeholder": "​",
            "style": "IPY_MODEL_3a0693ae046a460aa2f7f3b81e6dd3f9",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "9ed38bc043ec4f6dacd76f8aa31bc10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a64a696434bb446882408ae3e2549784": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cc5db1891a04c958842a50fc7849a5d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ed38bc043ec4f6dacd76f8aa31bc10b",
            "value": 0
          }
        },
        "d3134e270fbf49cabcd526ee4e79cd2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_002c96027a07454f8a8707d8bae27f73",
              "IPY_MODEL_a64a696434bb446882408ae3e2549784",
              "IPY_MODEL_8fd0854c9fc244f7838b0a3b05063bff"
            ],
            "layout": "IPY_MODEL_6573a68b10644dabb7820866fbd41dd0"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
